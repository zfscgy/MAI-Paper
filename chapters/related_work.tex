In this section, we will introduce earlier researches on privacy preserving machine learning, including traditional privacy notions, cryptography-based private preserving machine learning methods, differential privacy and federated learning.

\subsection{Privacy Preserving Methods}
Privacy preserving during the data analysis process has long been concerned. The k-anonymity\cite{k-anonymity} method is to perturb or hide some of the attributes which can be used to identify individual records, so-called the quasi identifiers. Beyond it, there are l-diversity\cite{l-diversity} aiming for adding diversity in a group of 'close' records, and t-closeness\cite{t-closeness} aiming for make the distribution similar for different group of records. However, as the era of big data and machine learning comes, the amount of data become enormous and the structure of data is fairly complicated, which is kind of incompatible with those previous privacy notions. For example, machine learning tasks usually do not need quasi-identifiers i.e. phone numbers, postcodes. But with huge amount of data, even without any quasi identifiers, privacy can be leaked. \cite{Narayanan2006netflixleak} shows that even a few records exposed, the attacker may be able to locate a specific person in the database. More recently, \cite{fredrikson2015inversion} shows that even trained models can leak some information about the training data. And DeepLeakage\cite{zhuligeng2019deepleakage} shows the possibility to reconstruct training samples from gradients.

\subsection{Cryptography-based Methods}
There are two major cryptographic methods for privacy preserving machine learning. One is homomorphic encryption, which allows arithemtic operations to be performed on the encrypted data. Another is secure multiparty computing, which enables multiple parties to jointly evaluate a function while keeping their inputs private.

Cryptonets\cite{bachrach2016cryptonets} first applied the somewhat fully homomorphic encryption to deep neural network. All computations are done on the encrypted data. The authors tested this model on the MNIST dataset, and achieved 99\% accuracy with a throughput about 59000 prediction per hour and a latency for about 250 seconds. Gazelle\cite{juvekar2018gazelle} avoided expensive fully homomorphic encryption and used packed additive homomorphic encryption to improve efficiency, and used garbled circuit to calculate non-linear activations. It reduces single image classification latency to around 30 microseconds. GELU-Net\cite{zhangqiao2019gelunet} let clients to calculate the activations while server calculate the linear transformation using additive homomorphic encryption. 

Alongside the homomorphic encryption methods, secure multiparty computing methods are also widely used. Beaver\cite{beaver1991} used precomputed triples to accelerate online multiplication. 
ABY\cite{demmler2015aby} mixed arithmetic, boolean and Yao sharing together provided a efficient two-party computation protocol that supports various kinds of computations covered common machine learning functions. ABY3\cite{mohassel2018aby3} hugely improved its efficiency under 3PC setting. 
\newline
SecureML\cite{mohassel2017secureml} combined arithmetic sharing and garbled circuit together to perform linear regression, logistic regression and neural network. Chameleon\cite{riazi2018chameleon} improved ABY by third-party aided multiplication triple generation and adopting GMW protocols\cite{GMW1987}. SecureNN\cite{wagh2019securenn} uses four sharing schemes and customized protocols for private comparison, outperforms previous works on four convolutional networks.

Those methods all have their advantages: Homomorphic encryption requires only one-round communication, and the security is proven by cryptography; Multiparty computing is faster in most cases, and can be more efficient with semi-honest third party introduced; However, these methods still requires heavy computation and massive communication, and are often hard to implement since they requires modern cryptographic methods, which can be difficult for machine learning developers.

\subsection{Differential Privacy}
Differential privacy was proposed by \cite{dwork2014dp}. It protects privacy by limiting the change of function when one record in the data changed. Differential privacy is always achieved by adding noises somewhere in the data analysis process. For example, \cite{abadi2016deepdp} adds the noise in the gradients of training. PATE\cite{nicolas2017pate} applied differential privacy on the label-generation phase of teacher models. And the ESA architecture\cite{Bittau2017ESA} uses local differential privacy to ensure the worst-case privacy when all other parties are colluding together.
Differential privacy provided a strong tool to evaluate privacy, but it will certainly affects the model performance in machine learning since noises are added. And in our view, it's focused on the maximum effect of one record, but not the actual sensitive data. In other words, maybe we do not need such a strict standard in order to protect data privacy. 

\subsection{Federated learning}
Federated learning was first proposed by \cite{mcmahan17fed}. It trains the model in a decentralized manner by sending model parameters to user devices, then the model is trained locally on those devices. After that, server gathers the model updates and calculates the final update. In order to prevent privacy leakage by uploading model updates, secure aggregation\cite{bonawitz2017aggre} and homomorphic encryption\cite{phong2018additive} methods were proposed. However, federated learning methods are designed for horizontally split data, i.e. different data owners have different samples, but the features are overlapped. But sometimes, data can be vertically split, i.e. same samples, different features. And let user devices to train the model will certainly affects the efficiency and performance.