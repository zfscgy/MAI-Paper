This paper proposed a new privacy notion called reconstructive privacy. Unlike differential privacy, this privacy notion focuses on the probability of reconstructing useful information from the transformed data. And using this definition, we proved that simple random permutations and linear transformations are very hard to invert.

Based on this, those random transformations can be applied for private machine learning. Non-linear functions can then be computed easily without garbled circuits or other MPC protocols. More over, after the data transformed, third party computation servicers can perform computations on the transformed data. By comparing with Rosetta, CryptoNets and SecureNN, we showed that this method hugely reduces the computation and communication costs. 